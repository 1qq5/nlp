# 本节课笔记目录

- [理论学习](/02_1.md)
- [波士顿地铁路线规划](/02_2.md)
- [作业：北京地铁路线规划](/02_3.md)
- 作业：寻找最佳预测拟合模型（本文）

4. Mathematical or Analytical Based 基于数学或统计的模型

# 作业：寻找最佳预测拟合模型

很多机器学习的优化是通过数学运算完成的。比如我们希望能够找到最佳的线性回归。为了简化问题，这里我们以 `sklearn` 中的波士顿房价数据集中的 `RM` 变量与最终房价之间的关系为例。
```python
%matplotlib inline

from sklearn.datasets import load_boston
import matplotlib.pyplot as plt
import pandas as pd
import random

data = load_boston()
df = pd.DataFrame(data['data'], columns=data['feature_names'])
y = data['target']
X = df.RM # I choose RM column
```
如果我们画散点图，可以看到大概的线性拟合。

![](/pics/plot.png)

下面我们随机画一条直线，看看拟合情况：

![](/pics/plot_2.png)

拟合有够烂。假如我们画足够多条线，那么很可能会有一条线能够较好地拟合数据。这就是第一种算法：随机选择斜率和截距。

进行第一个算法之前，首先我们需要一种衡量模型性能的量度：损失函数，一般使用均方差或平均绝对值之差（进行平方或绝对值转换是为了消除差的正负性）。
```python
def RMSE(y, y_hat):
    return np.sqrt(sum((y - y_hat) ** 2) / len(y)) # 定义 RMSE

def MAE(y, y_hat):
    return sum(abs(y - y_hat)) / len(y) # 定义 MAE

def loss_random(X, y, n, loss=RMSE):
    loss_min = float('inf')
    k_best, b_best = 0, 0
    for i in range(n):
        k = random.random() * 200 - 100
        b = random.random() * 200 - 100
        y_hat = k * X + b
        loss_new = loss(y, y_hat)
        if loss_new < loss_min:
            loss_min = loss_new
            k_best, b_best = k, b
            print(f"round: {i}, k: {k_best}, b: {b_best}, {loss.__name__}: {loss_min}")
        
    return (k_best, b_best)
```
我们来运行一下。
```python
>>> loss_random(X, y, 2000)
round: 0, k: 39.78313539547378, b: -42.553520850823865, RMSE: 186.30287320208564
round: 2, k: 29.211778034979886, b: -29.68934722745702, RMSE: 132.28429197646585
round: 12, k: -25.151782735947364, b: 95.15302856649043, RMSE: 89.01301850593477
round: 16, k: 4.040032224742035, b: 16.895435863873317, RMSE: 21.128109463676168
round: 37, k: 14.988955665079956, b: -72.6918047710123, RMSE: 7.856493935804529
round: 671, k: 14.187521190895524, b: -66.69517029495044, RMSE: 7.506431951543031
round: 1268, k: 7.233520680658856, b: -25.68910395443052, RMSE: 7.276594291831291
```
因为斜率和截距是随机选择的，具有相当大的偶然性，因此损失函数的更新非常稀疏。有没有不是那么靠运气的算法呢？下面就要介绍第二种算法：监督优化的方向。这个方法选择一个随机的起始斜率和截距，然后沿四个方向 `(1, 1), (1, -1), (-1, 1), (-1, -1)` 依次改变，选取损失函数最小的方向改变斜率和截距，然后重复，一直到达到损失函数最小。
```python
def calculate_loss(X, y, n, alpha=0.01, loss=RMSE):
    '''
    calculate the loss for all 4 directions and select the smallest one
    '''
    loss_min = float('inf')
    direction = [(1, 1), (1, -1), (-1, 1), (-1, -1)]
    
    k = random.random() * 200 - 100
    b = random.random() * 200 - 100
    
    for i in range(n):
        loss_complete = []
        best_data = []
        k_b = []
        for coord in direction:
            dr_k, dr_b = coord
            k_new = k + dr_k * alpha
            b_new = b + dr_b * alpha
            k_b.append((k_new, b_new))
            y_hat = k_new * X + b_new
            loss_complete.append(loss(y, y_hat))
        best_data = sorted(zip(loss_complete, k_b), key=lambda x: x[0])[0]
        loss_new, (k_n, b_n) = best_data
        if loss_new < loss_min:
            k, b = k_n, b_n
            k_best, b_best = k_new, b_new
            loss_min = loss_new
            print(f"round: {i}, k: {k}, b: {b}, {loss.__name__}: {loss_min}")
    return (k_best, b_best)
```
运行一下：
```python
>>> calculate_loss(X, y, 2000)
round: 0, k: -32.68289821690326, b: 83.31030562866745, RMSE: 147.71434158456455
round: 1, k: -32.67289821690326, b: 83.32030562866746, RMSE: 147.64162648015352
round: 2, k: -32.662898216903265, b: 83.33030562866746, RMSE: 147.5689118390715
...
round: 1997, k: -12.712898216900966, b: 103.04030562867754, RMSE: 16.686687720956236
round: 1998, k: -12.702898216900966, b: 103.03030562867754, RMSE: 16.682268006921056
round: 1999, k: -12.692898216900966, b: 103.02030562867753, RMSE: 16.67801752679068

(-12.712898216900966, 103.02030562867753)
```
可以看到几乎每次都有更新。这里要特别提一下 `alpha`（学习速率），因为到后面的时候趋近于最优值，因此 1 作为步长有点过大，乘以一个小数 alpha 有助于避免移动过度。这个方法仍然有不足：因为学习速率是固定的，开始的时候优化速度过低，接近结束的时候优化速度又变得过高。有没有什么办法避免呢？有，这就是梯度下降法（gradient descent）。什么是梯度下降法呢？我们首先来看一看当损失函数是均方差时候的损失函数：

$$ RMSE = \frac{1}{n}\sum{(y - (kx+b))^2} = \frac{1}{n}\sum(y^2 -2y(kx+b) + (kx+b)^2)) = \frac{1}{n}\sum{(y^2 - 2yxk - 2yb + k^2x^2 + 2kxb + b^2)}$$

假如我们把损失函数看成是 y 的函数，那么上面的式子本质上是一个二元一次方程。大家都知道，二元一次方程的曲线如下：
![](/pics/line_plot.png)
众所周知，二元一次方程在斜率为 0 处有最小值，这就意味着此处的损失函数最小。因此我们只需要对损失函数求导数并找到最小值点的截距（b）和斜率（k）即可。注意，导数与损失函数的选择有关。假如我们希望找到均方差作为损失函数时的最小值，只需要对上面的方程分别以 k 和 b 求导数即可。

$$ \frac{\partial{_{loss}}}{\partial{_k}} = \frac{2}{n}(-y + kx + b)x = \frac{2}{n}(-y + \hat{y})x $$

$$ \frac{\partial{_{loss}}}{\partial{_b}} = \frac{2}{n}(-y + kx + b) = \frac{2}{n}(-y + \hat{y}) $$

即为 k 和 b 的导数式。有了导数式，再定义一个学习速率 alpha，我们就可以进行梯度下降的运算了。首先定义两个导数：
```python
def partial_k(x, y, y_hat):
    n = len(y)
    gradient = 0
    for x_i, y_i, y_hat_i in zip(list(x), list(y), list(y_hat)):
        gradient += (y_i - y_hat_i) * x_i
    return -2 / n * gradient

def partial_b(y, y_hat):
    n = len(y)
    gradient = 0
    for y_i, y_hat_i in zip(list(y), list(y_hat)):
        gradient += (y_i - y_hat_i)
    return -2 / n * gradient
```
然后仿照上面监督优化的思路写代码：
```python
def gradient(X, y, n, alpha=0.01, loss=RMSE):
    loss_min = float('inf')
    
    k = random.random() * 200 - 100
    b = random.random() * 200 - 100
    
    for i in range(n):
        y_hat = k * X + b
        loss_new = RMSE(y, y_hat)
        if loss_new < loss_min:
            loss_min = loss_new
        k_gradient = partial_k(X, y, y_hat)
        b_gradient = partial_b(y, y_hat)
        k += -k_gradient * alpha
        b += -b_gradient * alpha
    return (k, b)
```
看一看拟合的结果：
![](/pics/fitted_plot.png)
还不错吧？虽然我没有打印出拟合的过程，可以看到拟合到 12000 次以后（设置拟合 20000 次），提升的幅度就很小了。因此我们可以给梯度下降函数设置一个提升的阈值，小于这个阈值就停止梯度下降。这个功能怎么加就留给读者自己思考了。最后，我再尝试推导一下 MAE（平均绝对值之差）的梯度下降（不一定对...请读者指点）。MAE 的方程如下：

$$ MAE = \frac{1}{n}\sum{|y - \hat{y}|} = \frac{1}{n}{\sum{|y - (kx+b)}|} $$

因此 k 和 b 的导数如下：

$$ \frac{\partial{loss}}{\partial{k}} = \frac{1}{n}{\sum{\left\{
\begin{array}{rcl}
-x       &      & {y - \hat{y} > 0}\\
x     &      & {y - \hat{y} < 0}
\end{array} \right.}} $$

$$ \frac{\partial{loss}}{\partial{b}} = \frac{1}{n}{\sum {\left\{
\begin{array}{rcl}
-1       &      & {y - \hat{y} > 0}\\
1     &      & {y - \hat{y} < 0}
\end{array} \right.}} $$

直接代入即可得：
```python
def partial_k(x, y, y_hat):
    n = len(x)
    gradient = 0
    for xi, y_i, y_hat_i in zip(x, y, y_hat):
        if y_i - y_hat_i > 0:
            gradient += xi
        else:
            gradient -= xi
    return -1 / n * gradient

def partial_b(y, y_hat):
    n = len(y)
    gradient = 0
    for y_i, y_hat_i in zip(y, y_hat):
        if y_i - y_hat_i > 0:
            gradient += 1
        else:
            gradient -= 1
    return -1 / n
```
沿用上面的梯度下降函数即可。